import requests
import re
import json
import time
import os
import faiss
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer

# --- 1. è¨­å®šå€¤ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰ ---
# âš ï¸ ãƒãƒ¼ãƒ åã‚’ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã®ã¿ã«ä¿®æ­£ã—ã¦ãã ã•ã„
ESA_TEAM_NAME = "cs18a"
ESA_ACCESS_TOKEN = "Id6WLrpYfGhF8-l0MsuMI--55xUwn3JfTYmzXVZWpHo"

# URLã¯f-stringã§æ­£ã—ãæ§‹ç¯‰ã•ã‚Œã¾ã™
ESA_API_BASE_URL = f"https://api.esa.io/v1/teams/{ESA_TEAM_NAME}"
HEADERS = {
    "Authorization": f"Bearer {ESA_ACCESS_TOKEN}",
    "Content-Type": "application/json",
}

# --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š ---
CACHE_FILE = "esa_data_cache.json"
FAISS_INDEX_FILE = "esa_faiss_index.bin"
CACHE_EXPIRY_SECONDS = 24 * 60 * 60  # 1æ—¥ = 86400ç§’

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---


def clean_markdown(markdown_text: str) -> str:
    """
    Markdown è¨˜æ³•ã‚’ç°¡æ˜“çš„ã«é™¤å»ã—ã€ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚
    """
    # è¦‹å‡ºã—ã€ãƒªãƒ³ã‚¯ã€ç”»åƒã‚’å‰Šé™¤
    text = re.sub(r"#{1,6}\s?", "", markdown_text)  # è¦‹å‡ºã—
    text = re.sub(r"\[.*?\]\(.*?\)", "", text)  # ãƒªãƒ³ã‚¯ã¨ç”»åƒ
    text = re.sub(
        r"(\*\*|__|~~|\*|_|`)", "", text
    )  # å¤ªå­—ã€ã‚¤ã‚¿ãƒªãƒƒã‚¯ã€ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰
    text = re.sub(r"^\s*[-*+]\s", "", text, flags=re.MULTILINE)  # ãƒªã‚¹ãƒˆ
    text = re.sub(r"\n{2,}", "\n", text)  # é€£ç¶šã™ã‚‹æ”¹è¡Œã‚’ä¸€ã¤ã«
    return text.strip()


def simple_text_splitter(text: str, chunk_size: int = 500) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã‚µã‚¤ã‚ºã§ç°¡æ˜“çš„ã«ãƒãƒ£ãƒ³ã‚¯ï¼ˆåˆ†å‰²ï¼‰ã™ã‚‹ã€‚
    """
    chunks = []
    # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã‘ã‚Œã°åˆ†å‰²
    if text:
        for i in range(0, len(text), chunk_size):
            chunks.append(text[i : i + chunk_size])
    return chunks


# --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ ---


def check_cache_validity() -> bool:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€æœ‰åŠ¹æœŸé™å†…ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    if not os.path.exists(CACHE_FILE) or not os.path.exists(FAISS_INDEX_FILE):
        return False

    file_mtime = os.path.getmtime(CACHE_FILE)
    time_since_last_update = time.time() - file_mtime

    return time_since_last_update < CACHE_EXPIRY_SECONDS


def load_from_cache(
    embedding_model: SentenceTransformer,
) -> Tuple[List[Tuple[str, Dict]], faiss.Index]:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã¨FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€"""
    print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")

    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    documents_with_metadata = [(item["chunk"], item["metadata"]) for item in data]

    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰
    index = faiss.read_index(FAISS_INDEX_FILE)

    return documents_with_metadata, index


def save_to_cache(documents_with_metadata: List[Tuple[str, Dict]], index: faiss.Index):
    """å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä¿å­˜ã™ã‚‹"""

    # 1. JSONã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¿å­˜
    cache_data = []
    for chunk, metadata in documents_with_metadata:
        cache_data.append({"chunk": chunk, "metadata": metadata})

    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=4)

    # 2. FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜
    faiss.write_index(index, FAISS_INDEX_FILE)

    print(f"ğŸ’¾ è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã¨FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


# --- è¨˜äº‹ã®å–å¾—ã¨å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³é–¢æ•° ---


def fetch_esa_documents(
    embedding_model: SentenceTransformer,
) -> Tuple[List[Tuple[str, Dict]], faiss.Index]:
    """
    esa APIã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã€RAGã«å¿…è¦ãªå½¢å¼ã«å‡¦ç†ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    """

    # 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒã‚§ãƒƒã‚¯
    if check_cache_validity():
        try:
            return load_from_cache(embedding_model)
        except Exception as e:
            print(
                f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚APIã‹ã‚‰å†å–å¾—ã—ã¾ã™ã€‚"
            )

    # --- 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹ãªãŸã‚ã€APIã‹ã‚‰æ–°è¦å–å¾— ---
    all_documents = []
    page = 1

    while True:
        try:
            list_url = f"{ESA_API_BASE_URL}/posts"
            params = {"per_page": 100, "page": page, "include": "tags,category"}
            response = requests.get(list_url, headers=HEADERS, params=params)

            if response.status_code != 200:
                print(f"--- è‡´å‘½çš„ãª API ã‚¨ãƒ©ãƒ¼ ---")
                print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
                print(
                    f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {response.json().get('error', response.text)}"
                )
                # èªè¨¼ã‚„URLã®ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã€ã“ã“ã§å–å¾—ã‚’ä¸­æ­¢
                break

            posts_data = response.json()
            posts = posts_data.get("posts", [])

            if not posts:
                break

            print(f"--- ãƒšãƒ¼ã‚¸ {page}: {len(posts)} ä»¶ã®è¨˜äº‹ã‚’å‡¦ç†ä¸­ ---")

            for post in posts:
                post_number = post["number"]

                # è¨˜äº‹è©³ç´°ã‚’å–å¾—ã—ã€Markdownæœ¬æ–‡ã‚’å–å¾—
                detail_url = f"{ESA_API_BASE_URL}/posts/{post_number}"
                detail_response = requests.get(detail_url, headers=HEADERS)
                detail_response.raise_for_status()
                post_detail = detail_response.json()

                body_md = post_detail.get("body_md", "")

                # Markdownã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                plain_text = clean_markdown(body_md)
                chunks = simple_text_splitter(plain_text)

                # RAGç”¨ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã«è¿½åŠ 
                for chunk in chunks:
                    metadata = {
                        "source": post["url"],
                        "title": post["full_name"],
                        "category": post.get("category"),
                        "tags": post.get("tags"),
                    }
                    all_documents.append((chunk, metadata))

            page += 1
            if posts_data.get("next_page") is None:
                break

        except requests.exceptions.RequestException as e:
            print(f"API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            break
        except Exception as e:
            print(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
            break

    # --- 3. å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ ---
    if not all_documents:
        print(
            "âš ï¸ è­¦å‘Š: esa APIã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚"
        )
        # ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        dummy_chunk = "esaã‹ã‚‰æƒ…å ±ãŒå–å¾—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        dummy_embedding = embedding_model.encode([dummy_chunk])
        dimension = dummy_embedding.shape[1]
        dummy_index = faiss.IndexFlatL2(dimension)
        dummy_index.add(dummy_embedding)
        return [(dummy_chunk, {"source": "No Data"})], dummy_index

    documents = [d[0] for d in all_documents]
    doc_embeddings = embedding_model.encode(documents)

    dimension = doc_embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(doc_embeddings)

    # 4. æ–°ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¿å­˜
    save_to_cache(all_documents, index)

    print(
        f"--- å‡¦ç†å®Œäº†: {len(all_documents)} å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ ---"
    )
    return all_documents, index
